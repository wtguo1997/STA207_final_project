# -*- coding: utf-8 -*-
"""classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sl6vgNFVyoWZ0DqujFcXqhxthhMylQmm
"""

from google.colab import drive
drive.mount('/content/drive')

# load data
import pandas as pd

sessions = pd.read_csv('/content/drive/Shareddrives/207/prediction/prediction.csv')

print(sessions.describe())

sessions.groupby(['session..ID...contrast_left','session..ID...contrast_right']).size().reset_index().rename(columns={0:'count'})

sessions.iloc[0]

sessions.to_numpy().shape

# convert to numpy and split data into training and testing sets
x_train = sessions.to_numpy()[100:,1:5]
y_train = (sessions.to_numpy()[100:,5] + 1) // 2

x_test = sessions.to_numpy()[:100,1:5]
y_test = (sessions.to_numpy()[:100,5] + 1) // 2

from sklearn import svm
from sklearn import metrics
import matplotlib.pyplot as plt
import numpy as np

clf = svm.SVC(probability=True)
clf.fit(x_train, y_train)
y_pred = clf.predict_proba(x_test)[:,1]
fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)
roc_auc = metrics.auc(fpr, tpr)
display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='SVM')
display.plot()
plt.show()

# G-Mean = sqrt(Sensitivity * Specificity)
# calculate the g-mean for each threshold
gmeans = np.sqrt(tpr * (1-fpr))
# find the index of the largest g-mean
ix = np.argmax(gmeans)
print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
print('Best specificity=%f, sensitivity=%.3f' % (1-fpr[ix], tpr[ix]))
y_pred_thred = (y_pred >= thresholds[ix]).astype(int)
print(metrics.classification_report(y_test, y_pred_thred))
plt.plot([0,1], [0,1], linestyle='--', label='No Skill')
plt.plot(fpr, tpr, marker='.', label='SVM')
plt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.show()

from sklearn.linear_model import LogisticRegression
from sklearn import metrics
import matplotlib.pyplot as plt

clf = LogisticRegression()
clf.fit(x_train, y_train)
y_pred = clf.predict_proba(x_test)[:,1]
fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)
roc_auc = metrics.auc(fpr, tpr)
display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='LogisticRegression')
display.plot()
plt.show()

# G-Mean = sqrt(Sensitivity * Specificity)
# calculate the g-mean for each threshold
gmeans = np.sqrt(tpr * (1-fpr))
# find the index of the largest g-mean
ix = np.argmax(gmeans)
print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
print('Best specificity=%f, sensitivity=%.3f' % (1-fpr[ix], tpr[ix]))
y_pred_thred = (y_pred >= thresholds[ix]).astype(int)
print(metrics.classification_report(y_test, y_pred_thred))
# plot the roc curve for the model
plt.plot([0,1], [0,1], linestyle='--', label='No Skill')
plt.plot(fpr, tpr, marker='.', label='LR')
plt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')
# axis labels
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
# show the plot
plt.show()

from sklearn.linear_model import RidgeClassifier
from sklearn import metrics
import matplotlib.pyplot as plt

clf = RidgeClassifier(alpha=0.06)
clf.fit(x_train, y_train)
y_pred = clf._predict_proba_lr(x_test)[:,1]
fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)
roc_auc = metrics.auc(fpr, tpr)
display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='RidgeClassifier')
display.plot()
plt.show()

# G-Mean = sqrt(Sensitivity * Specificity)
# calculate the g-mean for each threshold
gmeans = np.sqrt(tpr * (1-fpr))
# find the index of the largest g-mean
ix = np.argmax(gmeans)
print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
print('Best specificity=%f, sensitivity=%.3f' % (1-fpr[ix], tpr[ix]))
y_pred_thred = (y_pred >= thresholds[ix]).astype(int)
print(metrics.classification_report(y_test, y_pred_thred))
# plot the roc curve for the model
plt.plot([0,1], [0,1], linestyle='--', label='No Skill')
plt.plot(fpr, tpr, marker='.', label='Ridge')
plt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.show()

from pprint import pprint
n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 5)]
max_features = ['auto', 'sqrt']
max_depth = [int(x) for x in np.linspace(10, 110, num = 5)]
max_depth.append(None)
min_samples_split = [2, 5, 10]
min_samples_leaf = [1, 2, 4]
bootstrap = [True, False]
random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}
pprint(random_grid)

from sklearn.model_selection import RandomizedSearchCV
##rf = RandomForestClassifier()
##rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 5, verbose=1, random_state=42, n_jobs=-1)
##rf_random.fit(x_train, y_train)
##rf_random.best_params_

"""Fitting 5 folds for each of 100 candidates, totalling 500 fits \\
{'n_estimators': 325, \\
 'min_samples_split': 10, \\
 'min_samples_leaf': 4, \\
 'max_features': 'sqrt', \\
 'max_depth': 10, \\
 'bootstrap': True}
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics
import matplotlib.pyplot as plt

#Random forest with best parameters
clf = RandomForestClassifier(n_estimators=325, max_depth=10, n_jobs=-1, verbose=0, min_samples_leaf=4, min_samples_split=10, max_features='sqrt', bootstrap=True)
clf.fit(x_train, y_train)
y_pred = clf.predict_proba(x_test)[:,1]
fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)
roc_auc = metrics.auc(fpr, tpr)
display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='RandomForestClassifier')
display.plot()
plt.show()

# G-Mean = sqrt(Sensitivity * Specificity)
# calculate the g-mean for each threshold
gmeans = np.sqrt(tpr * (1-fpr))
# find the index of the largest g-mean
ix = np.argmax(gmeans)
print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
print('Best specificity=%f, sensitivity=%.3f' % (1-fpr[ix], tpr[ix]))
y_pred_thred = (y_pred >= thresholds[ix]).astype(int)
print(metrics.classification_report(y_test, y_pred_thred))
plt.plot([0,1], [0,1], linestyle='--', label='No Skill')
plt.plot(fpr, tpr, marker='.', label='RandomForest')
plt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.show()

from sklearn.neural_network import MLPClassifier
from sklearn import metrics
import matplotlib.pyplot as plt

clf = MLPClassifier(max_iter=450, learning_rate='adaptive')
clf.fit(x_train, y_train)
y_pred = clf.predict_proba(x_test)[:,1]
fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)
roc_auc = metrics.auc(fpr, tpr)
display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='MLPClassifier')
display.plot()
plt.show()

# G-Mean = sqrt(Sensitivity * Specificity)
# calculate the g-mean for each threshold
gmeans = np.sqrt(tpr * (1-fpr))
# find the index of the largest g-mean
ix = np.argmax(gmeans)
print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
print('Best specificity=%f, sensitivity=%.3f' % (1-fpr[ix], tpr[ix]))
y_pred_thred = (y_pred >= thresholds[ix]).astype(int)
print(metrics.classification_report(y_test, y_pred_thred))
plt.plot([0,1], [0,1], linestyle='--', label='No Skill')
plt.plot(fpr, tpr, marker='.', label='MLP')
plt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.show()

